{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a33308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색할 키워드를 입력해주세요:스타들의 일상\n",
      "\n",
      "크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):1\n",
      "\n",
      "크롤링할 시작 페이지:  1 페이지\n",
      "\n",
      "크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):200\n",
      "\n",
      "크롤링할 종료 페이지:  200 페이지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3141/3141 [00:00<00:00, 2763068.13it/s]\n",
      "100%|██████████| 1141/1141 [06:35<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 기사 갯수: 총  2000 개\n",
      "중복 제거 후 행 개수:  967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/suminbae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "끝\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# 페이지 url 형식에 맞게 바꾸어 주는 함수 만들기\n",
    "  #입력된 수를 1, 11, 21, 31 ...만들어 주는 함수\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num+1\n",
    "    else:\n",
    "        return num+9*(num-1)\n",
    "\n",
    "# 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "\n",
    "def makeUrl(search, start_pg, end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(start_page)\n",
    "        #print(\"생성url: \", url)\n",
    "        return url\n",
    "    else:\n",
    "        urls = []\n",
    "        for i in range(start_pg, end_pg + 1):\n",
    "            page = makePgNum(i)\n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        #print(\"생성url: \", urls)\n",
    "        return urls    \n",
    "\n",
    "# html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "def news_attrs_crawler(articles,attrs):\n",
    "    attrs_content=[]\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "#html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    #html 불러오기\n",
    "    original_html = requests.get(i,headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver,'href')\n",
    "    return url\n",
    "\n",
    "\n",
    "#####뉴스크롤링 시작#####\n",
    "\n",
    "#검색어 입력\n",
    "search = input(\"검색할 키워드를 입력해주세요:\")\n",
    "#검색 시작할 페이지 입력\n",
    "page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):\")) # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 시작 페이지: \",page,\"페이지\")   \n",
    "#검색 종료할 페이지 입력\n",
    "page2 = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):\")) # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 종료 페이지: \",page2,\"페이지\")   \n",
    "\n",
    "\n",
    "# naver url 생성\n",
    "url = makeUrl(search,page,page2)\n",
    "\n",
    "#뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url =[]\n",
    "news_contents =[]\n",
    "news_dates = []\n",
    "for i in url:\n",
    "    url = articles_crawler(url)\n",
    "    news_url.append(url)\n",
    "\n",
    "\n",
    "#제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "\n",
    "    \n",
    "#제목, 링크, 내용 담을 리스트 생성\n",
    "news_url_1 = []\n",
    "\n",
    "#1차원 리스트로 만들기(내용 제외)\n",
    "makeList(news_url_1,news_url)\n",
    "\n",
    "#NAVER 뉴스만 남기기\n",
    "final_urls = []\n",
    "for i in tqdm(range(len(news_url_1))):\n",
    "    if \"news.naver.com\" in news_url_1[i]:\n",
    "        final_urls.append(news_url_1[i])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# 뉴스 내용 크롤링\n",
    "\n",
    "for i in tqdm(final_urls):\n",
    "    #각 기사 html get하기\n",
    "    news = requests.get(i,headers=headers)\n",
    "    news_html = BeautifulSoup(news.text,\"html.parser\")\n",
    "\n",
    "    # 뉴스 제목 가져오기\n",
    "    title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    if title == None:\n",
    "        title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "    \n",
    "    # 뉴스 본문 가져오기\n",
    "    content = news_html.select(\"div#dic_area\")\n",
    "    if content == []:\n",
    "        content = news_html.select(\"#articeBody\")\n",
    "\n",
    "    # 기사 텍스트만 가져오기\n",
    "    # list합치기\n",
    "    content = ''.join(str(content))\n",
    "\n",
    "    # html태그제거 및 텍스트 다듬기\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "    content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "    pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "    content = content.replace(pattern2, '')\n",
    "\n",
    "    news_titles.append(title)\n",
    "    news_contents.append(content)\n",
    "\n",
    "    try:\n",
    "        html_date = news_html.select_one(\"div#ct> div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "        news_date = html_date.attrs['data-date-time']\n",
    "    except AttributeError:\n",
    "        news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "        news_date = re.sub(pattern=pattern1,repl='',string=str(news_date))\n",
    "    # 날짜 가져오기\n",
    "    news_dates.append(news_date)\n",
    "\n",
    "print(\"검색된 기사 갯수: 총 \",(page2+1-page)*10,'개')\n",
    "# print(\"\\n[뉴스 제목]\")\n",
    "# print(news_titles)\n",
    "# print(\"\\n[뉴스 링크]\")\n",
    "# print(final_urls)\n",
    "# print(\"\\n[뉴스 내용]\")\n",
    "# #print(news_contents)\n",
    "\n",
    "# print('news_title: ',len(news_titles))\n",
    "# print('news_url: ',len(final_urls))\n",
    "# print('news_contents: ',len(news_contents))\n",
    "# print('news_dates: ',len(news_dates))\n",
    "\n",
    "# ###데이터 프레임으로 만들기###\n",
    "# import pandas as pd\n",
    "\n",
    "# #데이터 프레임 만들기\n",
    "# news_df = pd.DataFrame({'date':news_dates,'title':news_titles,'link':final_urls,'content':news_contents})\n",
    "\n",
    "# #중복 행 지우기\n",
    "# news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "# print(\"중복 제거 후 행 개수: \",len(news_df))\n",
    "\n",
    "# #데이터 프레임 저장\n",
    "# now = datetime.datetime.now() \n",
    "# news_df.to_csv(\"economic_data.csv\",encoding='utf-8-sig',index=False)\n",
    "# news_df.to_csv('{}_{}.csv'.format(search,now.strftime('%Y%m%d_%H시%M분%S초')),encoding='utf-8-sig',index=False)\n",
    "\n",
    "\n",
    "###데이터 프레임으로 만들기###\n",
    "import pandas as pd\n",
    "\n",
    "#데이터 프레임 만들기\n",
    "news_df = pd.DataFrame({'date':news_dates,'title':news_titles,'link':final_urls,'content':news_contents})\n",
    "\n",
    "#중복 행 지우기\n",
    "news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "print(\"중복 제거 후 행 개수: \",len(news_df))\n",
    "\n",
    "#데이터 프레임 저장\n",
    "now = datetime.datetime.now() \n",
    "news_df.to_csv(\"mbc처리전_data.csv\",encoding='utf-8-sig',index=False)\n",
    "#news_df.to_csv('{}_{}.csv'.format(search,now.strftime('%Y%m%d_%H시%M분%S초')),encoding='utf-8-sig',index=False)\n",
    "\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = txt.replace('\\n',' ')\n",
    "    txt = txt.replace('\\r',' ')    \n",
    "    txt = txt.replace('=','')\n",
    "    txt = txt.replace('\\\"','')   \n",
    "    txt = txt.replace('\\'','')\n",
    "    txt = txt.replace(',','')\n",
    "    txt = txt.replace('..','')\n",
    "    txt = txt.replace('...','')\n",
    "    txt = txt.replace('”','')\n",
    "    txt = txt.replace('.','. ')\n",
    "    txt = txt.replace('  ',' ')\n",
    "    txt = txt.replace('  ',' ')    \n",
    "    txt = txt.replace('  ',' ')   \n",
    "    txt = txt.replace('  ',' ')           \n",
    "    txt = txt.replace('  ',' ')\n",
    "    txt = txt.replace('  ',' ')    \n",
    "    txt = txt.replace('  ',' ')   \n",
    "    txt = txt.replace('  ',' ')\n",
    "    txt = txt.replace('[','')\n",
    "    txt = txt.replace(']','')\n",
    "    return txt.strip()\n",
    "\n",
    "\n",
    "news_df['content'] = news_df['content'].apply(clean_text)\n",
    "news_df['title'] = news_df['title'].apply(clean_text)\n",
    "\n",
    "pattens = [\"[34569][0-9]{3}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}\",\n",
    "           \"[0-9]{2,3}[\\:\\s\\;.\\;,\\;-;)][0-9]{3,4}[\\:\\s\\;.\\;,\\;-][0-9]{4}\",\n",
    "           \"[0-9]{1}[0-9]{1}[\\W]?[0-1]{1}[0-9]{1}[\\W]?[0-3]{1}[\\W]?[0-9]{1}[\\W]?[1-4]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}\",\n",
    "           \"[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{3}[\\:\\s\\;.\\;,\\;-]([0-9]{5,6}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{5}|[0-9]{2,3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{4,6}[\\:\\s\\;.\\;,\\;-][0-9]|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{2}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7})|[0-9]{4}[\\:\\s\\;.\\;,\\;-]([0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9])|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5,6}\"\n",
    "           ]\n",
    "\n",
    "filters = []\n",
    "for p in pattens:\n",
    "    filters.append(re.compile(p))\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentences = []\n",
    "\n",
    "for raw_text in news_df['content']:\n",
    "    try:\n",
    "        docs = nltk.sent_tokenize(clean_text(raw_text))\n",
    "        #print(docs)\n",
    "        for txt in docs:\n",
    "            if txt.find('▶') > -1 or txt.find('@') > -1 or txt.find('ⓒ') > -1: \n",
    "                pass\n",
    "            else:\n",
    "                txt = txt.strip()\n",
    "                if len(txt) > 40:\n",
    "                    #ok = True\n",
    "                    #for f in filters:\n",
    "                    #    if len(f.findall(txt)) > 0:\n",
    "                    #        ok = False\n",
    "                    #        print('fitered:',txt)\n",
    "                    #if ok:\n",
    "                    if any(chr.isdigit() for chr in txt):\n",
    "                        pass\n",
    "                    else:\n",
    "                        sentences.append(txt)\n",
    "    except KeyboardInterrupt as ki:\n",
    "        raise ki        \n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])    \n",
    "        \n",
    "        \n",
    "df3 = pd.DataFrame({'sentence':sentences})\n",
    "df3.to_csv(\"entertainment_data/스타들의일상_크롤링센텐스.csv\")\n",
    "print(\"끝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c108a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f74387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d732a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581df7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41faebce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f807e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4848d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8bd7eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 후 행 개수:  436\n"
     ]
    }
   ],
   "source": [
    "###데이터 프레임으로 만들기###\n",
    "import pandas as pd\n",
    "\n",
    "#데이터 프레임 만들기\n",
    "news_df = pd.DataFrame({'date':news_dates,'title':news_titles,'link':final_urls,'content':news_contents})\n",
    "\n",
    "#중복 행 지우기\n",
    "news_df = news_df.drop_duplicates(keep='first',ignore_index=True)\n",
    "print(\"중복 제거 후 행 개수: \",len(news_df))\n",
    "\n",
    "#데이터 프레임 저장\n",
    "now = datetime.datetime.now() \n",
    "news_df.to_csv(\"환율처리전_data.csv\",encoding='utf-8-sig',index=False)\n",
    "#news_df.to_csv('{}_{}.csv'.format(search,now.strftime('%Y%m%d_%H시%M분%S초')),encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af546852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 전처리\n",
    "def clean_text(txt):\n",
    "    txt = txt.replace('\\n',' ')\n",
    "    txt = txt.replace('\\r',' ')    \n",
    "    txt = txt.replace('=','')\n",
    "    txt = txt.replace('\\\"','')   \n",
    "    txt = txt.replace('\\'','')\n",
    "    txt = txt.replace(',','')\n",
    "    txt = txt.replace('..','')\n",
    "    txt = txt.replace('...','')\n",
    "    txt = txt.replace('”','')\n",
    "    txt = txt.replace('.','. ')\n",
    "    txt = txt.replace('  ',' ')\n",
    "    txt = txt.replace('  ',' ')    \n",
    "    txt = txt.replace('  ',' ')   \n",
    "    txt = txt.replace('  ',' ')           \n",
    "    txt = txt.replace('  ',' ')\n",
    "    txt = txt.replace('  ',' ')    \n",
    "    txt = txt.replace('  ',' ')   \n",
    "    txt = txt.replace('  ',' ')\n",
    "    txt = txt.replace('[','')\n",
    "    txt = txt.replace(']','')\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c0257e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df['A'] = df['A'].apply(add_2)\n",
    "\n",
    "news_df['content'] = news_df['content'].apply(clean_text)\n",
    "news_df['title'] = news_df['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600cee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "news_df['content'] = news_df['content'].apply(clean_text)\n",
    "news_df['title'] = news_df['title'].apply(clean_text)\n",
    "\n",
    "pattens = [\"[34569][0-9]{3}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}[\\;.\\;-\\; ][0-9]{4}\",\n",
    "           \"[0-9]{2,3}[\\:\\s\\;.\\;,\\;-;)][0-9]{3,4}[\\:\\s\\;.\\;,\\;-][0-9]{4}\",\n",
    "           \"[0-9]{1}[0-9]{1}[\\W]?[0-1]{1}[0-9]{1}[\\W]?[0-3]{1}[\\W]?[0-9]{1}[\\W]?[1-4]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}[\\W]?[0-9]{1}\",\n",
    "           \"[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{3}[\\:\\s\\;.\\;,\\;-]([0-9]{5,6}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{5}|[0-9]{2,3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{4,6}[\\:\\s\\;.\\;,\\;-][0-9]|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{2}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{4}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{3}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{7})|[0-9]{4}[\\:\\s\\;.\\;,\\;-]([0-9]{3}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9])|[0-9]{5}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{6}|[0-9]{6}[\\:\\s\\;.\\;,\\;-][0-9]{2}[\\:\\s\\;.\\;,\\;-][0-9]{5,6}\"\n",
    "           ]\n",
    "\n",
    "filters = []\n",
    "for p in pattens:\n",
    "    filters.append(re.compile(p))\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentences = []\n",
    "\n",
    "for raw_text in news_df['content']:\n",
    "    try:\n",
    "        docs = nltk.sent_tokenize(clean_text(raw_text))\n",
    "        #print(docs)\n",
    "        for txt in docs:\n",
    "            if txt.find('▶') > -1 or txt.find('@') > -1 or txt.find('ⓒ') > -1: \n",
    "                pass\n",
    "            else:\n",
    "                txt = txt.strip()\n",
    "                if len(txt) > 40:\n",
    "                    #ok = True\n",
    "                    #for f in filters:\n",
    "                    #    if len(f.findall(txt)) > 0:\n",
    "                    #        ok = False\n",
    "                    #        print('fitered:',txt)\n",
    "                    #if ok:\n",
    "                    if any(chr.isdigit() for chr in txt):\n",
    "                        pass\n",
    "                    else:\n",
    "                        sentences.append(txt)\n",
    "    except KeyboardInterrupt as ki:\n",
    "        raise ki        \n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])    \n",
    "        \n",
    "        \n",
    "df3 = pd.DataFrame({'sentence':sentences})\n",
    "df3.to_csv(\"환율_크롤링센텐스.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9179ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/suminbae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentences = []\n",
    "\n",
    "for raw_text in news_df['content']:\n",
    "    try:\n",
    "        docs = nltk.sent_tokenize(clean_text(raw_text))\n",
    "        #print(docs)\n",
    "        for txt in docs:\n",
    "            if txt.find('▶') > -1 or txt.find('@') > -1 or txt.find('ⓒ') > -1: \n",
    "                pass\n",
    "            else:\n",
    "                txt = txt.strip()\n",
    "                if len(txt) > 40:\n",
    "                    #ok = True\n",
    "                    #for f in filters:\n",
    "                    #    if len(f.findall(txt)) > 0:\n",
    "                    #        ok = False\n",
    "                    #        print('fitered:',txt)\n",
    "                    #if ok:\n",
    "                    if any(chr.isdigit() for chr in txt):\n",
    "                        pass\n",
    "                    else:\n",
    "                        sentences.append(txt)\n",
    "    except KeyboardInterrupt as ki:\n",
    "        raise ki        \n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94025166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'sentence':sentences})\n",
    "df3.to_csv(\"환율_크롤링센텐스.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000641f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ 사진 연합뉴스푸르밀 노동조합은 이번 희망 퇴직 공지에 반발하고 있습니다.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.iloc[10]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0895eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(\"환율_크롤링센텐스.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d01e2a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2270"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a813e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아그라왈 CEO 등 핵심 임원진 일괄 해고…장악력 강화 의도인듯 일론 머스크로이터 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>트위터 의사결정 구조의 최상층에 있는 기존 경영진을 축출 자신의 측근으로 물갈이해 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아그라왈은 머스크의 인수 선언 이후 스스로 레임덕 CEO라고 자조하며 불만을 표시해...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WP는 이런 조치를 두고 머스크가 세계에서 가장 영향력있는 소셜미디어 업체 중 하나...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>트위터의 샌프란시스코 본사를 방문한 일론 머스크머스크 트위터 계정 동영상 캡처.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>이러한 활동을 해당 지역본부뿐 아니라 중앙본부로도 전파하기 위한 노력도 게을리하지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>여성 경제인들의 글로벌 경쟁력 및 전국적인 비즈니스 네트워킹 강화 경영애로 해소 등...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2267</th>\n",
       "      <td>미래 여성 CEO 양성을 위한 ‘여성특성화고 학생 장학금 전달식’과 ‘청년 여성 C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>부대 행사로는 △울산광역시 여성기업 전시부스 운영 △여성경제인 애로상담 창구 운영 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269</th>\n",
       "      <td>이정한 여경협 회장은 여성 경제 활성화는 인구 고령화와 저성장 시대에 지속 가능한 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2270 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence\n",
       "0     아그라왈 CEO 등 핵심 임원진 일괄 해고…장악력 강화 의도인듯 일론 머스크로이터 ...\n",
       "1     트위터 의사결정 구조의 최상층에 있는 기존 경영진을 축출 자신의 측근으로 물갈이해 ...\n",
       "2     아그라왈은 머스크의 인수 선언 이후 스스로 레임덕 CEO라고 자조하며 불만을 표시해...\n",
       "3     WP는 이런 조치를 두고 머스크가 세계에서 가장 영향력있는 소셜미디어 업체 중 하나...\n",
       "4          트위터의 샌프란시스코 본사를 방문한 일론 머스크머스크 트위터 계정 동영상 캡처.\n",
       "...                                                 ...\n",
       "2265  이러한 활동을 해당 지역본부뿐 아니라 중앙본부로도 전파하기 위한 노력도 게을리하지 ...\n",
       "2266  여성 경제인들의 글로벌 경쟁력 및 전국적인 비즈니스 네트워킹 강화 경영애로 해소 등...\n",
       "2267  미래 여성 CEO 양성을 위한 ‘여성특성화고 학생 장학금 전달식’과 ‘청년 여성 C...\n",
       "2268  부대 행사로는 △울산광역시 여성기업 전시부스 운영 △여성경제인 애로상담 창구 운영 ...\n",
       "2269  이정한 여경협 회장은 여성 경제 활성화는 인구 고령화와 저성장 시대에 지속 가능한 ...\n",
       "\n",
       "[2270 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea163282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
